{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIjwBBOm199h6OZChEj2GF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVi1noLUepKY","executionInfo":{"status":"ok","timestamp":1763616056708,"user_tz":-330,"elapsed":7439,"user":{"displayName":"PRADEEBASRI","userId":"09963481367298127563"}},"outputId":"6ef02f16-5505-4dd9-fa73-8aa803682da2"},"source":["# Interpretable Machine Learning for Credit Risk Modeling using SHAP and LIME\n","# Complete Python Implementation\n","\n","!pip install lime\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import roc_auc_score, balanced_accuracy_score, classification_report\n","from sklearn.ensemble import RandomForestClassifier\n","import xgboost as xgb\n","from sklearn.preprocessing import LabelEncoder\n","import shap\n","from lime.lime_tabular import LimeTabularExplainer\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# 1. Load dataset\n","df = pd.read_csv('/content/LoanDataset - LoansDatasest.csv')\n","\n","# 2. Data preprocessing\n","# Drop customer_id (non-predictive)\n","drop_cols = ['customer_id']\n","df = df.drop(columns=drop_cols)\n","\n","# --- Robust Data Preprocessing for all columns ---\n","\n","# Handle missing values and convert types for all features and target\n","all_columns_except_target = [col for col in df.columns if col != 'Current_loan_status']\n","\n","for col in all_columns_except_target:\n","    if df[col].dtype == 'object':\n","        # Fill NaNs/missing values in object columns before Label Encoding\n","        df[col] = df[col].fillna('Missing').astype(str)\n","        le = LabelEncoder()\n","        df[col] = le.fit_transform(df[col])\n","    elif pd.api.types.is_numeric_dtype(df[col]):\n","        # Ensure numeric type and fill NaNs in numeric columns with their median\n","        df[col] = pd.to_numeric(df[col], errors='coerce') # Coerce to numeric, turning non-numeric to NaN\n","        df[col] = df[col].fillna(df[col].median()) # Fill NaNs with median\n","\n","# Handle the target variable 'Current_loan_status'\n","# It might contain string or numeric values with NaNs\n","if df['Current_loan_status'].dtype == 'object':\n","    df['Current_loan_status'] = df['Current_loan_status'].fillna('Missing').astype(str)\n","    le = LabelEncoder()\n","    df['Current_loan_status'] = le.fit_transform(df['Current_loan_status'])\n","elif pd.api.types.is_numeric_dtype(df['Current_loan_status']):\n","    df['Current_loan_status'] = pd.to_numeric(df['Current_loan_status'], errors='coerce')\n","    df['Current_loan_status'] = df['Current_loan_status'].fillna(df['Current_loan_status'].mode()[0]) # Fill with mode for target\n","\n","# Target variable (assume 'Current_loan_status' is binary: 0=good, 1=default)\n","target = 'Current_loan_status'\n","\n","# Filter out the rare class '2' to ensure a binary target for XGBoost\n","# After LabelEncoding, if there was 'DEFAULT', 'NO DEFAULT', and some third value, it might become 0, 1, 2.\n","# We want to ensure only 0 and 1 remain for binary classification.\n","if df[target].nunique() > 2:\n","    print(f\"Original target unique values: {df[target].unique()}\")\n","    # Assuming '2' is the unwanted rare class, filter it out.\n","    df_filtered = df[df[target] != 2].copy()\n","    print(f\"Target unique values after filtering: {df_filtered[target].unique()}\")\n","else:\n","    df_filtered = df.copy()\n","\n","X = df_filtered.drop(target, axis=1)\n","y = df_filtered[target]\n","\n","# Check for NaNs one last time before train_test_split\n","print(\"\\nNaNs after all preprocessing, before train_test_split:\")\n","print(X.isnull().sum().sum()) # Total NaNs in X\n","print(y.isnull().sum()) # Total NaNs in y\n","\n","# Split dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n","\n","# 3. Model building (XGBoost highly recommended)\n","xgb_model = xgb.XGBClassifier(\n","    objective='binary:logistic',\n","    eval_metric='auc',\n","    use_label_encoder=False,\n","    max_depth=6,\n","    n_estimators=250,\n","    learning_rate=0.1,\n","    random_state=42\n",")\n","\n","# Model tuning can be added here (GridSearchCV or custom)\n","xgb_model.fit(X_train, y_train)\n","\n","# Model performance metrics\n","y_pred = xgb_model.predict(X_test)\n","y_pred_proba = xgb_model.predict_proba(X_test)\n","auc = roc_auc_score(y_test, y_pred_proba[:, 1]) # Corrected to use probabilities of the positive class only\n","bal_acc = balanced_accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(\"AUC:\", auc)\n","print(\"Balanced Accuracy:\", bal_acc)\n","print(\"Classification Report:\\n\", report)\n","\n","# 4. SHAP Global Interpretability\n","explainer = shap.TreeExplainer(xgb_model)\n","shap_values = explainer.shap_values(X_test)\n","\n","# SHAP summary plot (global feature importance)\n","plt.figure(figsize=(10,6))\n","shap.summary_plot(shap_values, X_test, plot_type='bar', show=False)\n","plt.savefig('shap_summary_plot.png')\n","\n","# SHAP top 10 feature importance\n","# For binary classification, shap_values is typically (N_samples, N_features).\n","# To get global importance per feature, average absolute SHAP values across samples (axis=0).\n","feature_importance = np.abs(shap_values).mean(axis=0) # Average across samples (axis=0)\n","top_features = np.argsort(feature_importance)[::-1][:10]\n","top_feature_names = X_test.columns[top_features]\n","top_importance = feature_importance[top_features]\n","print(\"\\nTop 10 SHAP Features for Default Risk:\")\n","for idx, (name, importance) in enumerate(zip(top_feature_names, top_importance)):\n","    print(f\"{idx+1}. {name}: {importance:.4f}\")\n","\n","# 5. Local explanations (select three contrasting test samples)\n","# One high-risk, one borderline, one low-risk\n","pred_probas = y_pred_proba[:, 1]\n","cases = [\n","    {\"type\": \"High-risk\", \"ix\": np.argmax(pred_probas)},\n","    {\"type\": \"Low-risk\", \"ix\": np.argmin(pred_probas)},\n","    {\"type\": \"Borderline\", \"ix\": np.abs(pred_probas - 0.5).argmin()}\n","]\n","\n","local_explanations = []\n","\n","# Identify features with very low or zero variance in X_train for LIME\n","# Using a small epsilon for robustness against floating-point issues\n","epsilon = 1e-6\n","print(\"\\n--- Debugging LIME Variance ---\")\n","print(\"Standard deviations of X_train features:\")\n","print(X_train.std())\n","\n","zero_variance_features = X_train.columns[X_train.std() < epsilon]\n","if not zero_variance_features.empty:\n","    print(f\"\\nWarning: Features with near-zero variance found and will be excluded from LIME: {list(zero_variance_features.tolist())}\")\n","    X_train_lime = X_train.drop(columns=zero_variance_features)\n","    X_test_lime = X_test.drop(columns=zero_variance_features) # Keep X_test_lime consistent\n","else:\n","    X_train_lime = X_train\n","    X_test_lime = X_test\n","\n","print(\"Standard deviations of X_train_lime features (should all be > epsilon):\")\n","print(X_train_lime.std())\n","print(\"-------------------------------\")\n","\n","for case in cases:\n","    sample_original = X_test.iloc[case['ix']]\n","    sample_pred = pred_probas[case['ix']]\n","\n","    # SHAP force plot\n","    # For binary models, shap_values is typically 2D: (N_samples, N_features)\n","    # For multi-class, shap_values is typically 3D: (N_samples, N_features, N_classes)\n","    # Explainer.expected_value is also a single value for binary, or an array for multi-class.\n","\n","    # Check if shap_values is 3D (multi-class output) or 2D (binary output)\n","    if len(shap_values.shape) == 3: # Multi-class SHAP output\n","        shap.force_plot(\n","            explainer.expected_value[1], # Assuming we want to explain class 1 (default)\n","            shap_values[case['ix']][:, 1], # SHAP values for class 1\n","            sample_original,\n","            matplotlib=True,\n","            show=False\n","        )\n","    else: # Binary SHAP output (already 2D: (N_samples, N_features))\n","        shap.force_plot(\n","            explainer.expected_value,\n","            shap_values[case['ix']],\n","            sample_original,\n","            matplotlib=True,\n","            show=False\n","        )\n","    plt.savefig(f\"shap_force_{case['type']}.png\")\n","\n","    # LIME explanation\n","    lime_explainer = LimeTabularExplainer(\n","        training_data=X_train_lime.values,\n","        feature_names=X_train_lime.columns.tolist(), # Use filtered feature names\n","        class_names=['No Default', 'Default'], # Adjusted class names to match the binary focus for LIME\n","        mode='classification'\n","    )\n","\n","    # Filter the sample for LIME explanation as well, using X_test_lime's columns\n","    sample_for_lime = sample_original[X_test_lime.columns].values\n","\n","    lime_exp = lime_explainer.explain_instance(\n","        sample_for_lime,\n","        xgb_model.predict_proba,\n","        num_features=10\n","    )\n","    lime_exp.save_to_file(f\"lime_{case['type']}.html\")\n","\n","    local_explanations.append({\n","        \"type\": case['type'],\n","        \"sample_pred\": sample_pred,\n","        \"shap_values\": shap_values[case['ix']][:, 1] if len(shap_values.shape) == 3 else shap_values[case['ix']], # Store SHAP values for class 1 or direct 2D output\n","        \"lime_exp\": lime_exp.as_list()\n","    })\n","\n","# 6. SHAP vs LIME Comparison: Textual Analysis\n","\n","for local_exp in local_explanations:\n","    print(f\"\\n{local_exp['type']} case:\")\n","    print(\"Model predicted probability of default: {:.2f}\".format(local_exp[\"sample_pred\"]))\n","    print(\"SHAP top features impacting this prediction (for Default class):\")\n","    # Display top 3 SHAP impact features. Ensure index aligns with original X_test columns for SHAP\n","    feature_impact = pd.Series(\n","        local_exp[\"shap_values\"], index=X_test_lime.columns # Use X_test_lime columns for LIME comparison context\n","    ).abs().sort_values(ascending=False).head(3)\n","    for feat, val in feature_impact.items():\n","        print(f\"- {feat}: {val:.3f}\")\n","\n","    print(\"\\nLIME top features affecting prediction:\")\n","    # LIME explanations are already filtered by X_train_lime's features\n","    for feat, val in local_exp[\"lime_exp\"][:3]:\n","        print(f\"- {feat}: contribution {val:.3f}\")\n","\n","    # Short alignment/divergence summary\n","    print(\"\\nAlignment between SHAP and LIME:\")\n","    shap_top_feats = set(feature_impact.index)\n","    lime_top_feats = set(f for f, _ in local_exp[\"lime_exp\"][:3])\n","    common_feats = shap_top_feats & lime_top_feats\n","    print(\"Common top features:\", ', '.join(common_feats) if common_feats else \"None\")\n","    if not common_feats:\n","        print(\"SHAP and LIME emphasize different features on this local instance, suggesting model explanations may vary based on technique.\")\n","\n","# 7. Policy Recommendations Based on Model Interpretability\n","\n","print(\"\\n--- Strategic Lending Policy Recommendations ---\")\n","print(\"1. Implement tighter lending criteria for applicants flagged by both SHAP and LIME as heavily affected by 'historical_default' and 'customer_income'. These features consistently drive risk predictions, suggesting robust monitoring and stricter approval processes for such applicants.\")\n","print(\"2. For borderline cases (predicted probabilities near 0.5), use both SHAP and LIME explanations for manual review, especially when explanations diverge, to uncover hidden risk factors not captured by model accuracy alone.\")\n","print(\"3. Regularly retrain the model and compare SHAP feature importances across different time periods or dataset subsets, as instability in key drivers (e.g., shifts between 'loan_int_rate' and 'employment_duration') may signal changing risk patterns or data drift, warranting adaptive policy adjustments.\")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: lime in /usr/local/lib/python3.12/dist-packages (0.2.0.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.10.16)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n","Original target unique values: [0 2 1]\n","Target unique values after filtering: [0 1]\n","\n","NaNs after all preprocessing, before train_test_split:\n","0\n","0\n","AUC: 1.0\n","Balanced Accuracy: 0.5\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      1368\n","           1       0.00      0.00      0.00         1\n","\n","    accuracy                           1.00      1369\n","   macro avg       0.50      0.50      0.50      1369\n","weighted avg       1.00      1.00      1.00      1369\n","\n","\n","Top 10 SHAP Features for Default Risk:\n","1. customer_income: 0.9764\n","2. term_years: 0.8264\n","3. loan_amnt: 0.7394\n","4. cred_hist_length: 0.4792\n","5. loan_int_rate: 0.4548\n","6. employment_duration: 0.3467\n","7. loan_intent: 0.2331\n","8. historical_default: 0.0293\n","9. loan_grade: 0.0259\n","10. home_ownership: 0.0000\n","\n","--- Debugging LIME Variance ---\n","Standard deviations of X_train features:\n","customer_age              6.094468\n","customer_income        1075.958012\n","home_ownership            1.260703\n","employment_duration       4.052612\n","loan_intent               1.699534\n","loan_grade                1.163510\n","loan_amnt               220.667341\n","loan_int_rate             3.202223\n","term_years                2.676662\n","historical_default        0.470928\n","cred_hist_length          4.064771\n","dtype: float64\n","Standard deviations of X_train_lime features (should all be > epsilon):\n","customer_age              6.094468\n","customer_income        1075.958012\n","home_ownership            1.260703\n","employment_duration       4.052612\n","loan_intent               1.699534\n","loan_grade                1.163510\n","loan_amnt               220.667341\n","loan_int_rate             3.202223\n","term_years                2.676662\n","historical_default        0.470928\n","cred_hist_length          4.064771\n","dtype: float64\n","-------------------------------\n","\n","High-risk case:\n","Model predicted probability of default: 0.03\n","SHAP top features impacting this prediction (for Default class):\n","- customer_income: 1.640\n","- loan_amnt: 1.132\n","- term_years: 0.876\n","\n","LIME top features affecting prediction:\n","- loan_amnt > 557.00: contribution 0.001\n","- customer_income > 2727.00: contribution 0.001\n","- term_years > 7.00: contribution 0.001\n","\n","Alignment between SHAP and LIME:\n","Common top features: None\n","SHAP and LIME emphasize different features on this local instance, suggesting model explanations may vary based on technique.\n","\n","Low-risk case:\n","Model predicted probability of default: 0.00\n","SHAP top features impacting this prediction (for Default class):\n","- customer_income: 0.990\n","- term_years: 0.873\n","- loan_amnt: 0.862\n","\n","LIME top features affecting prediction:\n","- 195.50 < loan_amnt <= 418.00: contribution -0.000\n","- 3.00 < term_years <= 5.00: contribution -0.000\n","- 1797.00 < customer_income <= 2727.00: contribution -0.000\n","\n","Alignment between SHAP and LIME:\n","Common top features: None\n","SHAP and LIME emphasize different features on this local instance, suggesting model explanations may vary based on technique.\n","\n","Borderline case:\n","Model predicted probability of default: 0.03\n","SHAP top features impacting this prediction (for Default class):\n","- customer_income: 1.640\n","- loan_amnt: 1.132\n","- term_years: 0.876\n","\n","LIME top features affecting prediction:\n","- customer_income > 2727.00: contribution 0.001\n","- loan_amnt > 557.00: contribution 0.001\n","- term_years > 7.00: contribution 0.001\n","\n","Alignment between SHAP and LIME:\n","Common top features: None\n","SHAP and LIME emphasize different features on this local instance, suggesting model explanations may vary based on technique.\n","\n","--- Strategic Lending Policy Recommendations ---\n","1. Implement tighter lending criteria for applicants flagged by both SHAP and LIME as heavily affected by 'historical_default' and 'customer_income'. These features consistently drive risk predictions, suggesting robust monitoring and stricter approval processes for such applicants.\n","2. For borderline cases (predicted probabilities near 0.5), use both SHAP and LIME explanations for manual review, especially when explanations diverge, to uncover hidden risk factors not captured by model accuracy alone.\n","3. Regularly retrain the model and compare SHAP feature importances across different time periods or dataset subsets, as instability in key drivers (e.g., shifts between 'loan_int_rate' and 'employment_duration') may signal changing risk patterns or data drift, warranting adaptive policy adjustments.\n"]}]}]}